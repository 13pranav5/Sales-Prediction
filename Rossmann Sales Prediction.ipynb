{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rossman Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Predict 6 weeks of daily sales for 1,115 stores located across Germany.  \n",
    "Data: https://www.kaggle.com/c/rossmann-store-sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Situation: Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers \n",
    "    are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by \n",
    "    many factors, including promotions, competition, school and state holidays, seasonality, and locality. \n",
    "\n",
    "Desired Outcome: Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on whatâ€™s most important to them: their customers and their teams! \n",
    "\n",
    "Action: Build a supervised predictive models and compare them to find the best model with least RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "1) Load relavent libraries\n",
    "2) Import data\n",
    "3) Data cleaning\n",
    "4) Data transformation\n",
    "5) Feature Engineering\n",
    "6) Predictive Models\n",
    "7) Model Selection\n",
    "8) Hyper-parameter Tuning\n",
    "9) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) RandomSearch CV - Instead of using grid search alone, we can use random search to find the closest best parameter and then use the grid search to find the best hyperparameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "from math import sqrt\n",
    "import math\n",
    "import subprocess\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.ensemble as es\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = pd.read_csv('train.csv', parse_dates=[2])\n",
    "test_input  = pd.read_csv('test.csv',parse_dates=[3])\n",
    "store = pd.read_csv('store.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check dimensions of the input dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_input ', test_input.shape) #test  (41088, 8)\n",
    "print('train_input ',train_input.shape) #train  (1017209, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for Nulls in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Store variable null counts:\\n',store.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We see that there exists a lot of Nulls in the dataset which needs to be treated. We will replace the Null values with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge store level data with the test and train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_input,store,on='Store')\n",
    "test_df = pd.merge(test_input,store,on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check dimensions of the merged dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test ', test_df.shape) #test  (41088, 17)\n",
    "print('train ',train_df.shape) #train  (1017209, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for Nulls in the combined dataset.We see nulls in theopen column which we will have to treat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test variable null counts:\\n',test_df.isnull().sum()) \n",
    "print('\\nTrain variable null counts:\\n', train_df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping columns that are not required as predictor variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dropped = train_df.copy().drop(['Customers'],axis=1).fillna(1)\n",
    "test_df_dropped = test_df.copy().drop([\"Id\"],axis=1).fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Subset for open days and with store sales > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dropped = train_df_dropped[(train_df_dropped.Open != 0)&(train_df_dropped.Sales >0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The below function returns the week of the month for the specified date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_of_month(dt):\n",
    "    \"\"\".\n",
    "    \"\"\"\n",
    "    first_day = dt.replace(day=1)\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "    return int(math.ceil(adjusted_dom/7.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating date variables for train and test(Month, Year, Day, Week, WeekOfMonth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dropped['Month'] = train_df_dropped.Date.dt.month_name()\n",
    "train_df_dropped['Year'] = train_df_dropped.Date.dt.year\n",
    "train_df_dropped['Day'] = train_df_dropped.Date.dt.day_name()\n",
    "train_df_dropped['Week'] = train_df_dropped.Date.dt.week\n",
    "train_df_dropped['WeekOfMonth'] = train_df_dropped.Date.apply(week_of_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_dropped['Month'] = test_df_dropped.Date.dt.month_name()\n",
    "test_df_dropped['Year'] = test_df_dropped.Date.dt.year\n",
    "test_df_dropped['Day'] = test_df_dropped.Date.dt.day_name()\n",
    "test_df_dropped['Week'] = test_df_dropped.Date.dt.week\n",
    "test_df_dropped['WeekOfMonth'] = test_df_dropped.Date.apply(week_of_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculating promo2 open time in months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dropped['PromoOpen'] = 12 * (train_df_dropped['Year'] - train_df_dropped.Promo2SinceYear) + \\\n",
    "                                 (train_df_dropped.Week - train_df_dropped.Promo2SinceWeek) / 4.0\n",
    "train_df_dropped['PromoOpen'] = train_df_dropped['PromoOpen'].apply(lambda x: x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_dropped['PromoOpen'] = 12 * (test_df_dropped['Year'] - test_df_dropped.Promo2SinceYear) + \\\n",
    "                                (test_df_dropped.Week - test_df_dropped.Promo2SinceWeek) / 4.0\n",
    "test_df_dropped['PromoOpen'] = test_df_dropped['PromoOpen'].apply(lambda x: x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the monthly,weekly and daily avg sales historically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly Average\n",
    "def monthAvg(month):\n",
    "    return train_df_dropped.groupby(['Month'])['Sales'].mean()[month]\n",
    "\n",
    "#Daily Average\n",
    "def dayAvg(day,week):\n",
    "    return train_df_dropped.groupby(['Week', 'Day'])['Sales'].mean()[week,day]\n",
    "\n",
    "#Week Avg\n",
    "def weekAvg(week):\n",
    "    return train_df_dropped.groupby(['Week'])['Sales'].mean()[week]\n",
    "\n",
    "train_df_dropped['month_sales_avg'] = train_df_dropped.apply(lambda x: monthAvg(x['Month']),axis = 1)\n",
    "train_df_dropped['day_sales_avg'] = train_df_dropped.apply(lambda x: dayAvg(x['Day'],x['Week']),axis = 1)\n",
    "train_df_dropped['week_sales_avg'] = train_df_dropped.apply(lambda x: weekAvg(x['Week']),axis = 1)\n",
    "\n",
    "test_df_dropped['month_sales_avg'] = test_df_dropped.apply(lambda x: monthAvg(x['Month']),axis = 1)\n",
    "test_df_dropped['day_sales_avg'] = test_df_dropped.apply(lambda x: dayAvg(x['Day'],x['Week']),axis = 1)\n",
    "test_df_dropped['week_sales_avg'] = test_df_dropped.apply(lambda x: weekAvg(x['Week']),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the monthly,weekly and daily avg sales based on store type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly Average\n",
    "def monthAvgStore(StoreType, month):\n",
    "    return train_df_dropped.groupby(['StoreType','Month'])['Sales'].mean()[StoreType,month]\n",
    "\n",
    "#Daily Average\n",
    "def dayAvgStore(StoreType,day,week):\n",
    "    return train_df_dropped.groupby(['StoreType','Week', 'Day'])['Sales'].mean()[StoreType,week,day]\n",
    "\n",
    "#Week Avg\n",
    "def weekAvgStore(StoreType,week):\n",
    "    return train_df_dropped.groupby(['StoreType','Week'])['Sales'].mean()[StoreType,week]\n",
    "\n",
    "train_df_dropped['month_sales_avg_store'] = train_df_dropped.apply(lambda x: monthAvgStore(x['StoreType'],x['Month']),axis = 1)\n",
    "train_df_dropped['day_sales_avg_store'] = train_df_dropped.apply(lambda x: dayAvgStore(x['StoreType'],x['Day'],x['Week']),axis = 1)\n",
    "train_df_dropped['week_sales_avg_store'] = train_df_dropped.apply(lambda x: weekAvgStore(x['StoreType'],x['Week']),axis = 1)\n",
    "\n",
    "test_df_dropped['month_sales_avg_store'] = test_df_dropped.apply(lambda x: monthAvgStore(x['StoreType'],x['Month']),axis = 1)\n",
    "test_df_dropped['day_sales_avg_store'] = test_df_dropped.apply(lambda x: dayAvgStore(x['StoreType'],x['Day'],x['Week']),axis = 1)\n",
    "test_df_dropped['week_sales_avg_store'] = test_df_dropped.apply(lambda x: weekAvgStore(x['StoreType'],x['Week']),axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the monthly avg sales historically based on store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly Average\n",
    "def monthAvgSt(Store, month):\n",
    "    return train_df_dropped.groupby(['Store','Month'])['Sales'].mean()[Store,month]\n",
    "\n",
    "#Daily Average\n",
    "def dayAvgSt(Store,day,week):\n",
    "    return train_df_dropped.groupby(['Store','Week', 'Day'])['Sales'].mean()[Store,week,day]\n",
    "\n",
    "#Week Avg\n",
    "def weekAvgSt(Store,week):\n",
    "    return train_df_dropped.groupby(['Store','Week'])['Sales'].mean()[Store,week]\n",
    "\n",
    "train_df_dropped['month_sales_avg_st'] = train_df_dropped.apply(lambda x: monthAvgSt(x['Store'],x['Month']),axis = 1)\n",
    "train_df_dropped['day_sales_avg_st'] = train_df_dropped.apply(lambda x: dayAvgSt(x['Store'],x['Day'],x['Week']),axis = 1)\n",
    "train_df_dropped['week_sales_avg_st'] = train_df_dropped.apply(lambda x: weekAvgSt(x['Store'],x['Week']),axis = 1)\n",
    "\n",
    "test_df_dropped['month_sales_avg_st'] = test_df_dropped.apply(lambda x: monthAvgSt(x['Store'],x['Month']),axis = 1)\n",
    "test_df_dropped['day_sales_avg_st'] = test_df_dropped.apply(lambda x: dayAvgSt(x['Store'],x['Day'],x['Week']),axis = 1)\n",
    "test_df_dropped['week_sales_avg_st'] = test_df_dropped.apply(lambda x: weekAvgSt(x['Store'],x['Week']),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For hold-out evaluation, we are split the last 6 weeks of train dataset as hold-out set or validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sorted = train_df_dropped.sort_values(['Date'],ascending = False)\n",
    "train_df = train_sorted.copy()\n",
    "split_index = 6*7*1115\n",
    "valid_df = train_df[:split_index] \n",
    "train_final = train_df[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop variables to create the X variables to predict. We need to drop the store variable as there are too many stores to create dummy variables as this is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid_df.drop(['Sales',\"Date\",'Store'],axis=1)\n",
    "train = train_df.drop(['Sales',\"Date\",'Store'],axis=1)\n",
    "test = test_df_dropped.drop([\"Date\",'Store'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the datatypes of the final columns to check for any the categorical columns\n",
    "train_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining train,validation and test dataset to convert categorical variables to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs_num = len(train)\n",
    "valid_objs_num = len(valid)\n",
    "dataset = pd.concat(objs=[train, valid, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform non numeric columns to dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = [  'DayOfWeek' ,'StoreType','Assortment',  'PromoInterval','StateHoliday',  \n",
    "                     'Month', 'Day', 'Week','WeekOfMonth']\n",
    "dataset = pd.get_dummies(dataset ,columns = cols_to_transform )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming the dataset back to train, validation and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = (dataset[:train_objs_num])\n",
    "valid_final = (dataset[train_objs_num:valid_objs_num+train_objs_num])\n",
    "test_final = (dataset[valid_objs_num+train_objs_num:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining train and validation dataset for Nested Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_total = (dataset[:valid_objs_num+train_objs_num])\n",
    "train_final_total_sales = pd.concat(objs=[train_df['Sales'], valid_df['Sales']], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating target variables for train and test\n",
    "##### Taking log transformation of the predictor variable as the sales vary a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  np.log1p(train_df['Sales'])\n",
    "y_valid =  np.log1p(valid_df['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define evaluation metrics - based on the problem submissions the predictions are evaluated on the Root Mean Square Percentage Error (RMSPE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To determine which model to use, we compare the crossvalidation score of all the algorithms. We select the algorithm with least score and then we train the hyperparameters for this model to get the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will compare Decision tree regression, Knn regression, SVR, Random Forest regression and XgBoost regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run all the models we use a nested cross validation to select the best performing model. Creating a common function to calculate the nested cross validation score so we can chose our model and tune the hperparameters for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sel(alg, tuned_parameters, n1, n2, X, y):\n",
    "    inner_cv = KFold(n_splits=n1, shuffle=True,  random_state= 5)\n",
    "    outer_cv = KFold(n_splits=n2, shuffle=True,  random_state= 5)\n",
    "    clf = GridSearchCV(alg, tuned_parameters, cv = inner_cv)\n",
    "    print(clf.estimator)\n",
    "    nested_score = cross_val_score(clf, X, y, cv = outer_cv, scoring = 'mean_squared_error')\n",
    "    nested_score = np.sqrt(np.abs(nested_score))\n",
    "    RMSE = nested_score.mean()\n",
    "    STD = nested_score.std()\n",
    "    return RMSE, STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the results from above functions to select the model. In this example I i will tune hyperparameters for all the models. But the best way would be to choose a model and then tune hyperparameters for the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch and fitting the model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Hyper parameters of decision tree\n",
    "params = {'max_depth': np.arange(3, 10),\n",
    "                 'min_samples_split': np.linspace(0.01, 0.3, 30, endpoint=True), \n",
    "                 'min_samples_leaf': np.linspace(0.01, 0.15, 15, endpoint=True)}\n",
    "#Defining the model\n",
    "reg = DecisionTreeRegressor()\n",
    "#GridsearchCV to tune hyperparameters and get the best predictions \n",
    "grid_tree = GridSearchCV(reg, params, cv = 10, refit = True)\n",
    "#Training the model on train data\n",
    "grid_tree.fit(train_final,train_df['Sales'])\n",
    "#Predicting the sales on the validation dataset\n",
    "y_pred = grid_tree.predict(valid_final)\n",
    "#Evaluating the RMSE value\n",
    "rmspe(valid_df['Sales'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "tree_RMSE, tree_STD = model_sel(reg, params, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - K-NN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to normalize the x variables to implement Knn as this is a distance based algorithm and using pipeline to achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = scaler.fit_transform(train_final)\n",
    "X_valid = scaler.fit_transform(valid_final)\n",
    "X_test = scaler.fit_transform(test_final)\n",
    "\n",
    "# build regression pipeline\n",
    "pipeline = Pipeline([('normalize', Normalizer()),\n",
    "                     ('kbest', SelectKBest(f_classif)),\n",
    "                     ('regressor', KNeighborsRegressor())])\n",
    "knns = KNeighborsRegressor()\n",
    "# try knn__n_neighbors from 1 to 20, and feature count from 1 to len(features)\n",
    "parameters = {'kbest__k':  list(range(1, X_train.shape[1]+1)),\n",
    "              'regressor__n_neighbors': list(range(1,11))}\n",
    "grid = GridSearchCV(pipeline, parameters, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "grid.fit(X_train, train_df['Sales'])\n",
    "\n",
    "y_pred = grid.predict(X_valid)\n",
    "rmspe(valid_df['Sales'],y_pred)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "knn_RMSE, knn_STD = model_sel(pipeline, parameters, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as es\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf = es.RandomForestRegressor()\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = grid, cv = 10, verbose=10, n_jobs = -1)\n",
    "rf_grid.fit(train_final,train_df['Sales'])\n",
    "y_pred = rf_grid.predict(valid_final)\n",
    "rmspe(valid_df['Sales'],y_pred)\n",
    "\n",
    "y_pred = rf_grid.predict(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "rf_RMSE, rf_STD = model_sel(rf, grid, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=n_cols, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adamax')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=base_model, verbose=0)\n",
    "\n",
    "## Tuning Hyper -Parameter\n",
    "# Optimizing for Batch Size and Epochs\n",
    "batch_size = [5, 20, 40, 100]\n",
    "epochs = [10, 50, 100,300]\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = train_final.shape[1]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    scoring='neg_mean_squared_error',n_jobs=1,cv=10)\n",
    "grid_result = grid.fit(train_final,train_df['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing for Size of first and second layers\n",
    "def base_model(l1=200,l2=100):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(l1, input_dim=n_cols, kernel_initializer='normal' ,activation='relu'))\n",
    "    model.add(Dense(l2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adamax')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=base_model, verbose=0, epochs=10, batch_size=5)\n",
    "l1 = [60, 200, 500, 600, 1000]\n",
    "l2 = [100, 250 ,400, 500, 600, 700]\n",
    "param_grid = dict(l1=l1, l2=l2)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(train_final,train_df['Sales'])\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model on best hyper parameter setting\n",
    "def final_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=n_cols, kernel_initializer='normal' ,activation='relu'))\n",
    "    model.add(Dense(600, kernel_initializer='normal',activation='relu',))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adamax')\n",
    "    return model\n",
    "\n",
    "reg = KerasRegressor(build_fn=final_model, epochs=300, batch_size=40,verbose=1,validation_split=0.2)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "results = np.sqrt(-1*cross_val_score(reg, train_final,train_df['Sales'],scoring= \"neg_mean_squared_error\", cv=kfold))\n",
    "print(\"Training RMSE mean and std from CV: {} {}\".format(results.mean(),results.std()))\n",
    "\n",
    "# Prediction Evalution on testing data\n",
    "reg.fit(train_final,train_df['Sales'])\n",
    "y_pred = reg.predict(train_final)\n",
    "\n",
    "y_pred = reg.predict(valid_final)\n",
    "rmspe(valid_df['Sales'],y_pred)\n",
    "\n",
    "y_pred = reg.predict(test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection using Random Forest\n",
    "clf = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "clf.fit(train_final,train_df['Sales'])\n",
    "feature_importance = pd.DataFrame(np.round(clf.feature_importances_,3),\n",
    "                        index = X_train.columns,\n",
    "                        columns=['importance']).sort_values('importance',\n",
    "                        ascending = False)\n",
    "\n",
    "#Features are ranked based on how they reduce imprurity and create pure nodes.\n",
    "#Based on the ranking, we decide to pick up the top 10 features\n",
    "#which have importance greater than 0.009.\n",
    "\n",
    "#Based on the values, we take the top 10 features and compare how \n",
    "#performance differs.\n",
    "top_features = feature_importance.nlargest(10,\"importance\")\n",
    "top_features .T.plot(kind = \"bar\")\n",
    "\n",
    "# Selecting top features for training\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled_fs = scaler.fit_transform(X_train.filter(items = top_features.index))\n",
    "x_valid_scaled_fs = scaler.transform(X_test.filter(items = top_features.index))\n",
    "x_test_scaled_fs = scaler.transform(X_test.filter(items = top_features.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 - SVR\n",
    "\n",
    "#Grid Search SVM Regression\n",
    "parameters = {'kernel':['linear','rbf','poly'], 'C':[0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [0.1, 1, 10],'degree': [0, 1, 2]}\n",
    "clf = GridSearchCV(SVR(), parameters,cv= 10, verbose = 10,scoring='neg_mean_squared_error',refit=True,n_jobs=-1)\n",
    "clf.fit(x_train_scaled_fs,train_df['Sales']) \n",
    "clf.best_params_\n",
    "y_pred = clf.predict(x_valid_scaled_fs)\n",
    "rmspe(valid_df['Sales'],y_pred)\n",
    "# Five submission attempt\n",
    "y_pred = clf.predict(x_test_scaled_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "svr_RMSE, svr_STD = model_sel(SVR(), parameters, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6 - GBM\n",
    "# Setting the hyperparameters by cross-validation\n",
    "gbm_parameters = [{'n_estimators': [100,500,1000],\n",
    "                    'max_depth': [6,10,14],\n",
    "                    'max_features': [4,8,12],\n",
    "                    'min_samples_leaf': [5,10,15]}]\n",
    "\n",
    "reg = GridSearchCV(GradientBoostingRegressor(), gbm_parameters, cv=10, scoring = 'neg_mean_squared_error')\n",
    "reg.fit(train_final,train_df['Sales'])\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(reg.best_params_)\n",
    "\n",
    "print(\"Best score found on development set:\")\n",
    "print(reg.best_score_)\n",
    "\n",
    "y_pred = reg.predict(valid_final)\n",
    "rmspe(valid_df['Sales'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "gbm_RMSE, gbm_STD = model_sel(GradientBoostingRegressor(), gbm_parameters, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7 - Adaboost\n",
    "# Setting the hyperparameters by cross-validation\n",
    "ab_parameters = [{'n_estimators': [10,50,100,200],\n",
    "                    'learning_rate': [0.01,0.1,0.3]}]\n",
    "\n",
    "reg = GridSearchCV(AdaBoostRegressor(), ab_parameters, cv=10, scoring='neg_mean_squared_error')\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(reg.best_params_)\n",
    "\n",
    "print(\"Best score found on development set:\")\n",
    "print(reg.best_score_)\n",
    "\n",
    "\n",
    "# Final Adaboost model accuracy using the hyper-parameters obtained from grid search\n",
    "y_pred = reg.predict(X_test)\n",
    "reg.score(X_test,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "adb_RMSE, adb_STD = model_sel(AdaBoostRegressor(), ab_parameters, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model 6 - XGBoost\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "xgb_model = XGBRegressor()\n",
    "parameters = {'objective':['reg:linear'],\n",
    "              'learning_rate': [0.02,0.03,0.04,0.05], #so called `eta` value\n",
    "              'max_depth': [6,7,8,9,10,11,12,13],\n",
    "              'subsample': [0.7,0.8,0.09,],\n",
    "              'colsample_bytree': [0.7,0.8,0.9]}\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, cv=10, verbose=2, refit=True)\n",
    "clf.fit(train_final,train_df['Sales']) \n",
    "clf.best_params_\n",
    "y_pred = clf.predict(valid_final)\n",
    "rmspe(valid_df['Sales'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested crossvalidation score\n",
    "xg_RMSE, xg_STD = model_sel(xgb_model, parameters, 5, 5, train_final_total, train_final_total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XgBoost found to be the best performing model compared all others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For XGboost since we are using DMatrix as input we will need to do a custom loss function to ensure that we can calculate RMSPE\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "\n",
    "# XGboost converting to DMatrix and using num_boost_round on the final selected parameters\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",  \n",
    "          \"eta\": 0.03,  \n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\": 0.9,\n",
    "          \"colsample_bytree\": 0.7\n",
    "          }\n",
    "num_boost_round = 4000\n",
    "\n",
    "dtrain = xgb.DMatrix(train_final, y_train)\n",
    "dvalid = xgb.DMatrix(valid_final, y_valid)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "xg_model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds= 100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "x_train_total = pd.concat(objs=[train_final, valid_final], axis=0)\n",
    "y_train_total = pd.concat(objs=[y_train, y_valid], axis=0)\n",
    "\n",
    "#Training the model on the entire data\n",
    "dtrain = xgb.DMatrix(x_train_total, y_train_total)\n",
    "dtest = xgb.DMatrix(test_final)\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",   \n",
    "          \"eta\": 0.03,   \n",
    "          \"max_depth\": 10,  \n",
    "          \"subsample\": 0.9, \n",
    "          \"colsample_bytree\": 0.7,        \n",
    "          }\n",
    "num_round = 1000\n",
    "xg_model = xgb.train(params, dtrain, num_round)\n",
    "# make predictionon test data\n",
    "preds = xg_model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Note that xgboost.train() will return a model from the last iteration, not the best one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
